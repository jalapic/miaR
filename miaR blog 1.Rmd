---
title: "miaR - working with data from the Austin Animal Center"
output: html_document
---
Date: March 11, 2021

Hi, for this blog, I will be working with a dataset from the Austin Animal Center in Austin, TX. The dataset is a running list of intakes dating back to 2013. Here is the link https://data.austintexas.gov/Health-and-Community-Services/Austin-Animal-Center-Intakes/wter-evkm
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
The first step is to load in necessary packages and the dataset. I downloaded the file onto my desktop first, but the data can retrieved from the link above.
```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(plyr)
library(lubridate)
df <- read_csv("data/austin_animal.csv")
```
Here are the first 6 rows of the datset:
```{r}
head(df)
```
Let's tidy up the column names:
```{r}
colnames(df) <- c("id", "name", "datetime", "monthyear", "location", 
                  "type", "condition", "animal", "sex", "age", "breed", "color",
                  "time", "date")
```
What I am first interested in has to do with the names of the intakes and whether there are any correlations between names and type of animal. The `name` column is a little bit messy, so I first have to tidy it up. Some of the name entries include '*' or the animal's weight in grams. gsub() can help eliminate these unwanted symbols, digits, and characters.
```{r, message = FALSE, warning = FALSE}
df$name <- tolower(df$name)
df$name <- gsub("\\*", "", df$name) 
df$name <- gsub("[0-9]+", "", df$name)
df$name <- gsub("grams", "", df$name)
```
Now, I can start answering some questions. First, what are the most common cat names?
I will use filter() to extract cats from the `animal` column. Because some names were not entered into the dataset, there are 'NA' entries. The code in line 42 takes these away. count() allows me to see these names but also how many cats have each name that is present. 
```{r}
cats <- df %>% filter(animal == "Cat") 
cats <- (cats[!is.na(cats$name),])
cats <- count(cats, "name") %>%
  arrange(desc(freq)) %>%
  head(20)
cats
```
***note to self: get rid of blank space

The five most common cat names for this dataset:
(1) Luna - 122,
(2) Charlie - 119,
(3) Lucy - 84,
(4) Bella - 83,
(5) Max - 83

Now, let's repeat the same process to answer the same question about dogs in the set.
```{r}
dogs <- df %>% filter(animal == "Dog") 
dogs <- (dogs[!is.na(dogs$name),])
dogs <- count(dogs, "name") %>%
  arrange(desc(freq)) %>%
  head(20)
dogs
```
The five most common dog names for this set:
(1) Max - 538,
(2) Bella - 472
(3) Luna - 413,
(4) Rocky - 373,
(5) Daisy - 362

Wow, I am surprised by these results. Out of the 5 most common cat and dog names, there are 3 overlaps (Max, Bella, Luna). I expected for there to be some noticeable differentiation between cat and dog names â€” at least enough to where there's not so much overlap between the most common names.

Now, let's make some visualizations.

This dataset has columns to mark when intakes were specifically brought in. These columns are `datetime` and `monthyear`. They happen to be identical. I'm not sure why they are repeats of one another. Let's take a look:
```{r}
z <- df[,3:4]
head(z)
```
First, I am going to make a plot showing the times throughout the day when animals are dropped off. Because column 3 combines the date with the time, I will use sub() to leave out the date and only be left with the time. I am also uninterested in the exact minute when animals were taken in, so I will use sub() once again to take the hour by itself and get rid of minutes and seconds. I will visualize this data by using the very basic plot().
```{r}
df$time <- sub("^\\S+\\s+", '', df$datetime)

hours <- sub("\\:.*", "", df$time) 

count(hours)
times <- table(hours)
plot(times, xlab = "time of day (hours)", ylab = "amount of intakes", main = "when are animals dropped off?")
```
===
The Austin Animal Center's open hours are from 11am - 6pm, so the increase in drop-offs during this range of time is expected.

Now, I am going to make another visualization answering a similar question: how many intakes are dropped off each day of the week? The Austin Animal Center is open every day, so let's see if the amount of intakes are evenly spread or if there's a day with more intakes than another.

I am using as.Date() and lubridate::wday to change the entries in the `datetime` column to only be dates, and to associate each date with a number which then corresponds to a day of the week. It's important to note here that '1' corresponds to Sunday, '2' corresponds to Monday, and so on. count() can show us the number of entries for every number (day of the week). Then, I used ggplot() to compile this data into a barchart.
```{r, results = 'hide'}
df$date <- as.Date(df$datetime,format = "%m/%d/%Y")

lubridate::wday(df$date)

days <- count(wday(df$date, label = TRUE, abbr = FALSE))
```
```{r}
ggplot(days, aes(x, freq)) +
  geom_col(fill = "lightblue", color = "navy", show.legend = FALSE) +
  xlab("days of the week") +
  ylab("amount of intakes") +
  ggtitle("days of the week") +
  ylim(0, 20000)
```
===
Let's take a turn and try working with maps. We'll need to load in some additional packages.
```{r, warning = F, message = F}
library(tidygeocoder)
library(data.table)
library(sp)
library(rgdal)
if(!require("osmdata")) install.packages("osmdata")
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("sf")) install.packages("sf")
if(!require("ggmap")) install.packages("ggmap")
```
I'm going to repeat the same process as earlier to separate dogs from the entire dataset and remove any 'NA' entries. Column 5 is `location` so I'm pulling it so it's the only column in the "dogs" dataframe.
```{r}
dogs <- df %>% filter(animal == "Dog")
dogs <- (dogs[!is.na(dogs$name),])
dogs <- dogs[,5]
dogs <- as.data.frame(dogs)
```
I decided that I would plot the 20 most common locations where dogs are brought in from. I used count() again to do this and I looked at the top 60, because some addresses were entered in as repeats because they were not formatted in the exact same way. I then copied and pasted the 20 most frequent locations into the format below by separating street, from city, from state.
```{r, results = 'hide'}
addr <- count(dogs) %>%
  arrange(desc(freq))

head(addr, 60)

adds <- data.frame(
  street = c("7201 Levander Loop", "4434 Frontier Trail", "1156 W Cesar Chavez", "12034 Research Blvd",
             "4106 N Lamar Blvd", "600 Barwood Park", "12138 N Lamar Blvd", "5701 Johnny Morris Rd", 
             "400 Grove Blvd", "5106 Village Square", "5801 Ainez Dr", "5200 Knight Cir", 
             "13021 Dessau Rd", "2100 Barton Springs Rd", "1300 Crossing Pl", "1800 E Stassney L",
             "12118 Walnut Park Crossing", "1700 Teri Rd", "25613 River Fern Ct", "7601 Daffan Ln" ),
  city = c("Austin","Austin","Austin", "Austin", "Austin", "Austin","Austin",
           "Austin", "Austin", "Austin", "Austin", "Austin", "Austin", "Austin", "Austin",
           "Austin", "Austin", "Austin", "Austin", "Austin"),
  state = c("Texas","Texas","Texas", "Texas", "Texas", "Texas", "Texas", "Texas", "Texas", "Texas",
            "Texas", "Texas", "Texas", "Texas", "Texas", "Texas", "Texas", "Texas", "Texas", "Texas")
  )
```
I then made a loop so that my results would be stored in a more organized way. R can know what to expect, which will make the process easier and quicker. I stored my results, and turned each address into it's respective lat/lon notation by using coordinates().
```{r}
results <- vector('list', nrow(adds))

for(i in 1:nrow(adds)){
  
  results[[i]] <- geo(street = adds[i,1], 
                      city = adds[i,2],
                      state = adds[i,3], 
                      method = "census")
  
}

do.call("rbind", results)
res <- do.call("rbind", results)
res <- (res[!is.na(res$lat),])
res = as.data.table(res)
coordinates(res) = c("lat", "long")
points <- coordinates(res)
points = as.data.frame(points)
```
4 of these addresses couldn't be turned into lat/lon coordinates, so I will plot the 16 most common locations were dogs are brought in from. I now need to gather a map of Austin in order to plot my points and have a frame of reference. I finally used ggmap() to place my points onto a map of Austin.
```{r, warning = F, message = F}
map <- get_map(getbb("Austin"), maptype = "roadmap")

ggmap(map) +
  geom_point(data = points,
               aes(x = long, y = lat), 
             color = "darkblue", 
             size = 1.5,
             alpha = 0.5)
```










